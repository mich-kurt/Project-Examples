{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language": "python",
    "story": {
      "auth_token": "I6IJmwnzGX6taC_dLccYtmImNk92Zij6lJx1WS9PGM0=",
      "authorship_tag": "AB",
      "chapters": 18,
      "name": "Finding Characters",
      "parser": {},
      "root": "https://github.com/habermanUIUC/CodeStories-lessons/blob/main/lessons/p4ds/cc/finding_characters",
      "tag": "p4ds:cc:finding_characters"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mich-kurt/Project-Examples/blob/main/Project_Text_Analysis_Finding_Characters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0y5yY_xUYib"
      },
      "source": [
        "#### **Introduction to Programming for Data Science using Python**\n",
        "\n",
        "#**Project 4: Text Analysis - Finding Characters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBQm3IKzUYjT"
      },
      "source": [
        "One of the goals of the Cliff Note Generator was to generate a list of characters in a novel. We can actually use our current skill set and include the techniques discussed in the ngrams lesson to extract (with a good level of accuracy) the main characters of a novel.\n",
        "\n",
        "We will also make some improvements with some of the parsing, cleaning, and preparation of the data. It would be best to read this entire lesson before doing any coding. Also note that this lesson is a bit different in that you will be responsible for more of the code writing. What is being specified is a minimum. We highly recommend that you decompose any complex processes into multiple functions.\n",
        "\n",
        "###**Preparation**\n",
        "Before doing anything, read through the entire set of directions first. You will get a sense of the restrictions and overall goals.\n",
        "\n",
        "###**Step 1**\n",
        "Fill in the functions from the previous lessons (ngrams and stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_dHP_RnUYjX"
      },
      "source": [
        "#\n",
        "# from Ngrams and Stopwords Lessons\n",
        "#\n",
        "\n",
        "# Copy & paste your code from the ngrams lesson\n",
        "import re\n",
        "import collections\n",
        "def read_text(filename):\n",
        "  with open(filename,'r') as fd:\n",
        "    txt = fd.read()\n",
        "\n",
        "  return txt\n",
        "\n",
        "def split_text_into_tokens(text):\n",
        "    f_list = []\n",
        "    pattern = r\"['A-Za-z]+-?['A-Za-z0-9]+\"\n",
        "\n",
        "    words = re.findall(pattern, text)\n",
        "\n",
        "    for word in words:\n",
        "        # Remove words with first and last apostrophes\n",
        "        #dogs'\n",
        "\n",
        "        if \"'\" in word[0] and \"'\"  in word[-1]:\n",
        "          f_list.append(word[1:-1])\n",
        "\n",
        "        else:\n",
        "          f_list.append(word)\n",
        "    return f_list\n",
        "def split_text_into_tokens_2(text):\n",
        "    f_list = []\n",
        "    pattern = r\"['A-Za-z]+-?['A-Za-z0-9]+\"\n",
        "\n",
        "    words = re.findall(pattern, text)\n",
        "\n",
        "    for word in words:\n",
        "        # Remove words with first and last apostrophes\n",
        "        #dogs'\n",
        "        word = normalize_token(word)\n",
        "        if \"'\" in word[0] and \"'\"  in word[-1]:\n",
        "          f_list.append(word[1:-1])\n",
        "\n",
        "        else:\n",
        "          f_list.append(word)\n",
        "    return f_list\n",
        "def bi_grams(tokens):\n",
        "\n",
        "  i = 1\n",
        "  l_1 = []\n",
        "  while i < len(tokens):\n",
        "    f_tup = (tokens[i - 1], tokens[i])\n",
        "    l_1.append(f_tup)\n",
        "    i += 1\n",
        "\n",
        "  return l_1\n",
        "def normalize_token(token):\n",
        "\n",
        "  cleaned_token = token.strip()\n",
        "  X = cleaned_token.replace(\"'\",\"\")\n",
        "  return X\n",
        "def top_n(tokens, n):\n",
        "  cnt = collections.Counter(tokens)\n",
        "\n",
        "  return cnt.most_common(n)\n",
        "\n",
        "# From stopwords lesson\n",
        "def remove_stop_words(tokens, stoplist):\n",
        "  f_list= []\n",
        "  for i in tokens:\n",
        "    if i.lower() not in stoplist:\n",
        "      f_list.append(i)\n",
        "  return f_list\n",
        "\n",
        "def load_stop_words(filename):\n",
        "  f_list = []\n",
        "  txt = read_text(filename)\n",
        "  txt = txt.split(\"\\n\")\n",
        "  for i in txt:\n",
        "    if \"'\" in i[0] or \"'\" in i[-1]:\n",
        "     f_list.append(i.replace(\"'\",\"\"))\n",
        "    else:\n",
        "      f_list.append(i)\n",
        "  return f_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 2 : Test your code.**\n",
        "The following should now work"
      ],
      "metadata": {
        "id": "REVW3_qH8AYL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAxYRDeyUYkI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f1bf4e-29b9-4348-e1fc-3de1c8f88d19"
      },
      "source": [
        "def demo_test():\n",
        "\n",
        "   text = read_text('lupin.txt')\n",
        "   stop = load_stop_words('stopwords.txt')\n",
        "\n",
        "   tokens  = split_text_into_tokens(text)\n",
        "   cleaned = remove_stop_words(tokens, stop)\n",
        "   grams = bi_grams(cleaned)\n",
        "\n",
        "   print(top_n(grams, 10))\n",
        "\n",
        "demo_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('said', 'Duke'), 403), (('said', 'Guerchard'), 302), (('said', 'Formery'), 126), (('said', 'Germaine'), 123), (('said', 'Lupin'), 119), (('said', 'Sonia'), 74), (('said', 'Victoire'), 55), (('Mademoiselle', 'Kritchnoff'), 45), (('said', 'millionaire'), 44), (('Duke', 'Charmerace'), 43)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the following:\n",
        "```\n",
        "[(('said', 'Duke'), 403), (('said', 'Guerchard'), 302), (('said', 'Formery'), 126), (('said', 'Germaine'), 123), (('said', 'Lupin'), 119), (('said', 'Sonia'), 74), (('said', 'Victoire'), 55), (('Mademoiselle', 'Kritchnoff'), 45), (('said', 'millionaire'), 44), (('Duke', 'Charmerace'), 43)]\n",
        "```\n",
        "\n",
        "Note how this compares to the output when we didn't account for the case of the stopwords."
      ],
      "metadata": {
        "id": "DVSvfSwNBCZ3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vMEG9LzUYkR"
      },
      "source": [
        "###**Finding the Characters**\n",
        "With this machinery in place, we are ready to find characters in a novel (I hope you are reading this with great anticipation) using different strategies. Each of the strategies has a function to implement that strategy.\n",
        "\n",
        "####**Method #1**\n",
        "One attribute (or feature) of the text we are analyzing is that proper nouns are capitalized. Let’s capitalize on this and find all single words in the text whose first character is an uppercase letter and the word is **not** a stop word.\n",
        "\n",
        "Create and define the function find_characters_v1(text, stoplist, top):\n",
        "\n",
        "* Tokenize and clean the text using the function split_text_into_tokens\n",
        "* Filter the tokens so it has no stop words in it (regardless of case). The parameter stoplist is the array returned from load_stop_words\n",
        "* Create a new list of tokens (keep the order) of words that are capitalized. You can test the first character of the token.\n",
        "* Return the top words as a list of tuples (the first element is the word, the second is the count)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAFHEtV9UYkf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c09ceb88-eea3-43a0-e41b-8cdf1b3b81d8"
      },
      "source": [
        "def find_characters_v1(text, stoplist, top):\n",
        "  tokens = split_text_into_tokens_2(text)\n",
        "  count  = collections.Counter()\n",
        "  for i in tokens:\n",
        "    if i.lower() not in stoplist:\n",
        "      if i.lower() != i:\n",
        "        count[i] += 1\n",
        "\n",
        "  return count.most_common(top)\n",
        "\n",
        "text = read_text('lupin.txt')\n",
        "stop = load_stop_words('stopwords.txt')\n",
        "v1 = find_characters_v1(text,stop, 15)\n",
        "print(v1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Duke', 727), ('Guerchard', 642), ('Lupin', 331), ('Formery', 258), ('Germaine', 228), ('Sonia', 226), ('Oh', 189), ('Yes', 164), ('Victoire', 160), ('Im', 127), ('Charolais', 124), ('Well', 103), ('Gournay-Martin', 96), ('Charmerace', 84), ('Ive', 82)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Lupin, you should get the following (the output is formatted for clarity):\n",
        "```\n",
        "text = read_text('lupin.txt')\n",
        "stop = load_stop_words('stopwords.txt')\n",
        "v1 = find_characters_v1(text,stop, 15)\n",
        "print(v1)\n",
        "```\n",
        "You should see:\n",
        "```\n",
        "('Duke', 727),\n",
        "('Guerchard', 642),\n",
        "('Lupin', 331),\n",
        "('Formery', 258),\n",
        "('Germaine', 228),\n",
        "('Sonia', 226),\n",
        "('Oh', 189),\n",
        "('Yes', 164),\n",
        "('Victoire', 160),\n",
        "('Charolais', 124)\n",
        "```\n",
        "\n",
        "Notice with this very simple method we found 11 characters in the top 15. You also found an Oh and a Yes too. You might be inclined to start fiddling with the stop-words. The one you could add is 'Duke' and 'Well' -- the interjection, since we know that word does not provide much content in this context. But as we mentioned in the stop words lesson, that's a dangerous game, since other novels might include some of these:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1BztBBMwk5FSLxTy37naTUzeODVwg12kd)\n",
        "\n",
        "Link to the above [image](https://drive.google.com/file/d/1BztBBMwk5FSLxTy37naTUzeODVwg12kd/view?usp=sharing)\n",
        "\n",
        "###**Method #2**\n",
        "Another feature of characters in a novel is that some of them have two names (Arsène Lupin).\n",
        "\n",
        "Create and define the following function:\n",
        "```\n",
        "find_characters_v2(text, stoplist, top)\n",
        "```\n",
        "* Tokenize and clean the text using the function split_text_into_tokens\n",
        "* Convert the list of tokens into a list of bigrams (using your bi_grams method)\n",
        "* Filter out all bigrams to keep only the ones where both words are capitalized (just the first character)\n",
        "* Neither word should (either lower or upper) be in stoplist\n",
        "* Remember stoplist could be an empty list\n",
        "* Return the top bigrams as a list of tuples: The first element is the bigram tuple, the second is the count\n",
        "\n",
        "\n",
        "Note that we are **not** removing the stopwords from the text. We are now using the stopwords to make decisions on the text. The stopwords lesson has more details on this as well."
      ],
      "metadata": {
        "id": "_MSV70s753PO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaIjrT2fUYkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2fecd4d-dd60-40e2-f698-619367c67830"
      },
      "source": [
        "def find_characters_v2(text, stoplist, top):\n",
        "  #splits word\n",
        "  split_words = split_text_into_tokens(text)\n",
        "  #turns words to bigrams\n",
        "  ngrams = bi_grams(split_words)\n",
        "  caps_ngrams = []\n",
        "  n_stoplist_words = []\n",
        "  count = collections.Counter()\n",
        "  #iterates through bigrams\n",
        "  for i in ngrams:\n",
        "    #checks if the first word and second words lower version doesnt equal\n",
        "    #upper version and if the word is in the stoplist\n",
        "    if i[0][0].lower() != i[0][0] and i[1][0].lower() != i[1][0] and i[0].lower() not in stoplist and i[1].lower() not in stoplist:\n",
        "      caps_ngrams.append(i)\n",
        "  for j in caps_ngrams:\n",
        "    count[j] += 1\n",
        "  count_1 = count.most_common(top)\n",
        "\n",
        "  return count_1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "text = read_text('lupin.txt')\n",
        "stop = load_stop_words('stopwords.txt')\n",
        "v2 = find_characters_v2(text,stop, 15)\n",
        "print(v2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('Mademoiselle', 'Kritchnoff'), 45), (('Duke', 'Yes'), 16), (('Duke', 'Oh'), 15), (('Formery', 'Yes'), 11), (('Guerchard', 'Oh'), 11), (('Guerchard', 'Yes'), 11), (('Mademoiselle', 'Gournay-Martin'), 10), (('South', 'Pole'), 8), (('Firmin', 'Firmin'), 8), (('Mademoiselle', 'Germaine'), 7), (('Mademoiselle', 'Sonia'), 7), (('Chief-Inspector', 'Guerchard'), 7), (('University', 'Street'), 6), (('Du', 'Buits'), 6), (('ARS', 'NE'), 6)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "With the text of Lupin, the following is the expected output of v2:\n",
        "\n",
        "```\n",
        "(('Mademoiselle', 'Kritchnoff'), 45),\n",
        "(('Duke', 'Yes'), 16),\n",
        "(('Duke', 'Oh'), 15),\n",
        "(('Formery', 'Yes'), 11),\n",
        "(('Guerchard', 'Oh'), 11),\n",
        "(('Guerchard', 'Yes'), 11),\n",
        "(('Mademoiselle', 'Gournay-Martin'), 10),\n",
        "(('South', 'Pole'), 8),\n",
        "(('Du', 'Buits'), 8),\n",
        "(('Firmin', 'Firmin'), 8)\n",
        "```\n",
        "\n",
        "For this book, this method is quite useless, as there aren't many double cases and you can see that the top ones are just matched weirdly. Capitalization can mess with simple algorithms and this is a clear example of how.\n",
        "\n",
        "Note: in order to match these outputs, use the collections.Counter class. Otherwise, it's possible that your version of sorting will handle those tuples with equal counts differently (unstable sorting).\n",
        "\n",
        "###**Titles, a short diversion**\n",
        "Another feature of characters is that many of them have a title (also called honorifics) precede them (Dr. Mr. Mrs. Miss. Ms. Rev. Prof. Sir. etc). We will look for bi-grams that have these titles. However, we will **not** hard code the titles (we won't specify which titles to look for). We will let the data tell us what the 'titles' are.\n",
        "\n",
        "Here's the process to use to self discover titles:\n",
        "\n",
        "* Let's define a title as a capital letter followed by 1 to 4 lower case letters followed by a period. This is not perfect, but it captures a good majority of possible titles.\n",
        "* Create a list named title_tokens whose text matches the above criteria (hint: use regular expressions) for example:\n",
        "title_tokens = regex1.findall(text)\n",
        "* Now we need to remove words that might have ended a sentence with those same title characteristics (e.g. Tom. Bill. Pat. Etc. ). These names could have been in a sentence like \"Please go Tom.\" Tom is **not** a title, but it would have been found by our definition.\n",
        "* Use the same definition for titles (above) but instead of ending with a period, the token must end with whitespace. The idea is that hopefully somewhere in the text the same name will appear but without a period. It’s very likely that you would encounter 'Tom' somewhere in the text without a period, but it’s unlikely that Mr., Mrs., Dr., etc would appear without a period. Let's call this list pseudo_titles.\n",
        "pseudo_titles = regex2.findall(text)\n",
        "* The set of titles is essentially the first list of tokens, title_tokens with all the tokens in the second set (pseudo_titles) removed. For example, the first list might have 'Dr.', 'Tom.' and 'Mr.' in it and the second set might have 'Tom' and 'Ted' in it. The final title list would be ['Dr', 'Mr'].\n",
        "\n",
        "Name your function get_titles that encapsulates the above logic; it should return a list of atitles, with only a few valid titles in that list:"
      ],
      "metadata": {
        "id": "1VxqqepO7YOp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75aGHITJUYkq"
      },
      "source": [
        "import re\n",
        "def get_titles(txt):\n",
        "    f_list = []\n",
        "    title_tokens = r'\\b[A-Z]+[a-z]{1,4}\\.'\n",
        "    pseudo_titles = r'\\b[A-Z]+[a-z]{1,4}\\s'\n",
        "    words = re.findall(title_tokens, txt)\n",
        "    words_2 = re.findall(pseudo_titles,txt)\n",
        "\n",
        "    for i in words:\n",
        "      i = i.replace(\".\",\" \")\n",
        "      if i not in words_2:\n",
        "        f_list.append(i.strip())\n",
        "\n",
        "\n",
        "    f_set = sorted(set(f_list))\n",
        "    return f_set\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = read_text('lupin.txt')\n",
        "titles = get_titles(text)\n",
        "print(titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykmqRXtoZCoq",
        "outputId": "c58fec4a-01d4-491e-dbcb-42c86ace1108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dyck', 'May', 'Mlle', 'Mr', 'Star', 'Ste']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have get_titles working, the following should work:\n",
        "```\n",
        "text = read_text('lupin.txt')\n",
        "titles = get_titles(text)\n",
        "print(sorted(titles))\n",
        "```\n",
        "You should get 6 computed titles in Lupin, again with only a few actual titles:\n",
        "```\n",
        "['Dyck', 'May', 'Mlle', 'Mr', 'Star', 'Ste']\n",
        "```\n",
        "\n",
        "Do not move forward until this is working.\n",
        "\n",
        "###**Method #3**\n",
        "Create and define the following function\n",
        "\n",
        "find_characters_v3(text, stoplist, top)\n",
        "\n",
        "* Tokenize and clean the text\n",
        "* Convert the list of tokens into a list of bigrams\n",
        "* Filter out all bigrams such that the first word in the bigram is a title and the second word is capitalized (hint: use the output of get_titles) **and** the second word (either lower or upper) should not be in stoplist\n",
        "* Return the top bigrams as a list of tuples: the first element is the bigram tuple, the second is the count"
      ],
      "metadata": {
        "id": "5G96djC28uaB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx1iMbPgUYkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a5022f6-a982-4ef5-9765-981bb492798d"
      },
      "source": [
        "def find_characters_v3(text, stoplist, top):\n",
        "  count = collections.Counter()\n",
        "  titles = ['Dyck', 'May', 'Mlle', 'Mr', 'Star', 'Ste']\n",
        "  split = split_text_into_tokens(text)\n",
        "  ngrams = bi_grams(split)\n",
        "  for i in ngrams:\n",
        "    if i[0] in titles and i[1].lower() not in stoplist:\n",
        "      count[i] += 1\n",
        "  return count.most_common(top)\n",
        "\n",
        "\n",
        "text = read_text('lupin.txt')\n",
        "stop = load_stop_words('stopwords.txt')\n",
        "v3 = find_characters_v3(text, stop, 5)\n",
        "print(v3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('Mlle', 'Germaine'), 2), (('Mlle', 'Kritchnoff'), 2), (('Mr', 'Inspector'), 2), (('Mlle', 'Gournay-Martin'), 1), (('Mlle', \"Germaine's\"), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Lupin, you should get the following:\n",
        "```\n",
        "(('Mlle', 'Germaine'), 2),\n",
        "(('Mlle', 'Kritchnoff'), 2),\n",
        "(('Mr', 'Inspector'), 2),\n",
        "(('Mlle', 'Gournay-Martin'), 1),\n",
        "(('Mlle', \"Germaine's\"), 1),\n",
        "(('Ste', 'Clotilde'), 1)\n",
        "```\n",
        "\n",
        "While this doesn't yield a lot of solid information, it does give you a basic understanding of the fundamentals of finding these kinds of specific words and the process behind them."
      ],
      "metadata": {
        "id": "5Txbnnlm9OJw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpkT3QwyUYkv"
      },
      "source": [
        "###**Machine Learning?**\n",
        "You may have heard of (and used) the NLTK Python library that’s a popular choice for processing text. These libraries include models that were built by processing large amounts of text. We will use both the NLTK and SpaCy NLP libraries to do something similar in another lesson. However, these libraries have models built from using large data sets to extract entities (called NER for named entity recognition). These entities include organizations, people, places, money.\n",
        "\n",
        "The models that were built essentially learned what features (like capitalization or title words) were important when analyzing text and came up with a model that attempts to do the same thing we did here. However, we hard coded the rules (use bigrams, remove stop words, look for capital letters, etc). This is sometimes referred to as a rule-based system. The analysis is built on manually crafted rules.\n",
        "\n",
        "In machine learning (sometimes referred to as an automatic system), some of the algorithms essentially learn what features are important (or can learn how much weight to apply to each feature) to build a model and then uses the model to classify tokens as named entities. The biggest issue is that these models could be built with a very different text source (e.g. journal articles or twitter feed) than what you are processing. Also the models themselves require a large set of resources (memory, cpu) that you may not have available. What you built in this lesson is efficient, fast and fairly accurate.\n",
        "\n",
        "In the follow-on course, you'll be able to build your own text-based models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffqu0iuJUYkx"
      },
      "source": [
        "##**Submission**\n",
        "\n",
        "After implementing all the functions and testing them please download the notebook as \"info407_project_text_analysis_finding_characters.py\" and submit to gradescope under \"Project - Text Analysis - Finding Characters\" assignment tab.\n",
        "\n",
        "**NOTES**\n",
        "\n",
        "* Be sure to use the function names and parameter names as given.\n",
        "* DO NOT use your own function or parameter names.\n",
        "* Your file MUST be named \"info407_project_text_analysis_finding_characters.py\".\n",
        "* Comment out any lines of code and/or function calls to those functions that produce errors.\n",
        "* Grading cannot be performed if any of these are violated."
      ]
    }
  ]
}